# Attention Mechanism

### What is the name of the machine learning technique that allows a neural network to focus on specific parts of an input sequence?

- [ ] Long Short-Term Memory (LSTM)
- [ ] Convolutional neural network (CNN)
- [ ] Encoder-decoder
- [X] Attention mechanism

### What is the advantage of using the attention mechanism over a traditional recurrent neural network (RNN) encoder-decoder?

- [ ] The attention mechanism is more cost-effective than a traditional RNN encoder-decoder.
- [ ] The attention mechanism requires less CPU threads than a traditional RNN encoder-decoder.
- [ ] The attention mechanism is faster than a traditional RNN encoder-decoder.
- [X] The attention mechanism lets the decoder focus on specific parts of the input sequence, which can improve the accuracy of the translation.

### What is the advantage of using the attention mechanism over a traditional sequence-to-sequence model?

- [ ] The attention mechanism reduces the computation time of prediction.
- [ ] The attention mechanism lets the model formulate parallel outputs.
- [ ] The attention mechanism lets the model learn only short term dependencies.
- [X] The attention mechanism lets the model focus on specific parts of the input sequence.

### What is the purpose of the attention weights?

- [ ] To generate the output word based on the input data alone.
- [ ] To incrementally apply noise to the input data.
- [ ] To calculate the context vector by averaging words embedding in the context.
- [X] To assign weights to different parts of the input sequence, with the most important parts receiving the highest weights.

### How does an attention model differ from a traditional model?

- [ ] The decoder only uses the final hidden state from the encoder.
- [ ] The traditional model uses the input embedding directly in the decoder to get more context.
- [ ] The decoder does not use any additional information.
- [X] Attention models pass a lot more information to the decoder.

### What are the two main steps of the attention mechanism?

- [ ] Calculating the attention weights and generating the output word
- [ ] Calculating the context vector and generating the attention weights
- [ ] Calculating the context vector and generating the output word
- [X] Calculating the attention weights and generating the context vector

### What is the name of the machine learning architecture that can be used to translate text from one language to another?

- [ ] Long Short-Term Memory (LSTM)
- [ ] Convolutional neural network (CNN)
- [ ] Neural network
- [X] Encoder-decoder